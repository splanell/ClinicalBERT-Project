{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e872f36d-d453-4564-b628-1eaa4b140bcc",
   "metadata": {},
   "source": [
    "# üè• Fall Risk Detection in Nursing Notes Using ClinicalBERT\n",
    "\n",
    "This project demonstrates the use of [ClinicalBERT](https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT) to classify nursing notes for fall risk. The dataset used is a simulated collection of EHR-style notes labeled for fall risk presence or absence.\n",
    "\n",
    "This project simulates how natural language processing (NLP) can assist in patient safety surveillance ‚Äî particularly in support of CMS-reportable fall-risk metrics.\n",
    "\n",
    "**Key Goals:**\n",
    "- Use a pretrained ClinicalBERT model for classification\n",
    "- Tokenize and format unstructured clinical text\n",
    "- Fine-tune on a simulated dataset of nursing documentation\n",
    "- Evaluate classification performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4639a1f-6a9c-4728-8c35-30a8cf1ab669",
   "metadata": {},
   "source": [
    "# Step 1: Load and Preview the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'dataset.csv' with your actual filename\n",
    "df = pd.read_csv(\"dataset_expanded.csv\")\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805cfad-c8c5-4159-8694-dc1b9a37997c",
   "metadata": {},
   "source": [
    "# Step 2: Load ClinicalBERT and Tokenize the Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def49fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load ClinicalBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512  # Explicitly define max_length\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebde07f-c6d0-4c43-86fa-8100ae489a8f",
   "metadata": {},
   "source": [
    "# Step 3: Load Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebece756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7c379-afd7-44f3-a11b-f0148acf698b",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the ClinicalBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Define a function to tokenize your dataset text column (replace 'text' if your column is named differently)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenizer to the entire dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove columns you don't need (like the original text) for training\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "\n",
    "# Set format for TensorFlow \n",
    "tokenized_dataset.set_format(type='tensorflow', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62efc175-ffde-4b17-ad9d-5235aaba425f",
   "metadata": {},
   "source": [
    "# Step 5: Prepare Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec76767-1b21-4ce3-9c70-2011cab14f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "# Split the dataset into training and test sets (80/20)\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6b1c6-09ed-4e31-8722-270e483515a1",
   "metadata": {},
   "source": [
    "# Step 6: Load ClinicalBERT and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c1722-2dde-44b4-8c45-d01535fad3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the ClinicalBERT model for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Define a simple compute_metrics function (optional but useful)\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Initialize the Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15538d1e-60a6-4002-a56c-71c7bc8dd9d0",
   "metadata": {},
   "source": [
    "# Step 7: Save and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16d480-e07f-45dd-974e-27b8e493ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "trainer.save_model(\"./clinicalbert_fall_risk_model\")\n",
    "\n",
    "# Evaluate the Model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39981c64-321d-4b00-888c-c04e5fa0178b",
   "metadata": {},
   "source": [
    "# Step 8: Tokenize and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055d0be-74fb-487a-84e9-4e796abbabe7",
   "metadata": {},
   "source": [
    "# Step 8.1: Load and Convert to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61f74e-6c2c-42fa-a73e-119faddb51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"dataset_expanded.csv\")  # adjust path as needed\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbe8d6-3dd2-414c-8cd2-a235ef259db2",
   "metadata": {},
   "source": [
    "# Step 8.2: Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd952ced-ca4d-4dd8-8585-3365f9014501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937849c5-4c81-4651-a0a3-82bdd1a73b52",
   "metadata": {},
   "source": [
    "# Step 8.3: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13960bd-ba0e-4e36-87d0-598e52bbdf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291da34-b96b-4bbe-bc14-f7ad2cc91f90",
   "metadata": {},
   "source": [
    "# Step 8.4: Model and Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f238eba-519f-4e85-99f6-8ad4583dfed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4e35c-c2b6-4f2c-8181-05546327fea9",
   "metadata": {},
   "source": [
    "# Step 8.5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f52438-2776-49d9-ae7b-e83507fb809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06901d3d-186c-4b64-bc28-a9f82dfea8b3",
   "metadata": {},
   "source": [
    "# Step 8.6: Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fe2b5-0533-4df4-a4d6-cfd89db7e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./clinicalbert_fallrisk_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91357f6e",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusion\n",
    "\n",
    "This notebook demonstrates how ClinicalBERT ‚Äî a transformer model pretrained on real clinical notes ‚Äî can be fine-tuned on simulated nursing documentation to classify fall risk. This method is promising for tasks such as:\n",
    "\n",
    "- Identifying documentation patterns tied to fall risks\n",
    "- Supporting CMS quality metric reporting (e.g., inpatient falls)\n",
    "- Assisting healthcare administrators in early warning or dashboard systems\n",
    "\n",
    "Next steps may include:\n",
    "- Expanding the dataset for better generalizability\n",
    "- Exploring multi-label classification (e.g., fall + pressure injury risk)\n",
    "- Deploying the model in a lightweight dashboard using Streamlit\n",
    "\n",
    "*Authored by Larry L. Splane, RN, MBA, VA-BC*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clinicalbert-env)",
   "language": "python",
   "name": "clinicalbert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
